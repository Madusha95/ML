# -*- coding: utf-8 -*-
"""CNN_23.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tgQE7wQK2rfDRqjITTE8YxOXf8u3ncLv
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from tensorflow.keras.utils import to_categorical
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Conv1D, BatchNormalization, Activation, Add, Multiply, GlobalMaxPooling1D, Dense, Dropout
from tensorflow.keras.regularizers import l2
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
import matplotlib.pyplot as plt
import joblib
import json

# Load the train and test datasets
train_file_path = '/content/all_ecg_80_percent_c.xlsx'
test_file_path = '/content/all_ecg_20_percent_c.xlsx'

train_df = pd.read_excel(train_file_path)
test_df = pd.read_excel(test_file_path)

# Assuming 'Type' is the target column and the rest are features
X_train = train_df.drop('Type', axis=1).values
y_train = train_df['Type'].values

X_test = test_df.drop('Type', axis=1).values
y_test = test_df['Type'].values

# Encode the labels
le = LabelEncoder()
y_train_encoded = le.fit_transform(y_train)
y_test_encoded = le.transform(y_test)

# Convert to categorical
y_train_categorical = to_categorical(y_train_encoded)
y_test_categorical = to_categorical(y_test_encoded)

# Reshape the input data for CNN (samples, timesteps, features)
X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)


# Define the attention_module function
def attention_module(input_tensor, filters):
    # Pre-processing ResNet block
    x = Conv1D(filters, kernel_size=3, padding='same', kernel_regularizer=l2(0.01))(input_tensor)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)

    # Trunk branch
    trunk = Conv1D(filters, kernel_size=3, padding='same', kernel_regularizer=l2(0.01))(x)
    trunk = BatchNormalization()(trunk)
    trunk = Activation('relu')(trunk)

    # Mask branch
    mask = Conv1D(filters, kernel_size=3, padding='same', kernel_regularizer=l2(0.01))(x)
    mask = BatchNormalization()(mask)
    mask = Activation('relu')(mask)
    mask = Conv1D(1, kernel_size=1, padding='same', activation='sigmoid')(mask)

    # Apply attention
    output = Multiply()([trunk, mask])
    output = Add()([output, trunk])

    return output

# Define the build_attention_cnn function
def build_attention_cnn(input_shape, num_classes):
    inputs = Input(shape=input_shape)

    # Initial Conv1D layers with regularization
    x = Conv1D(32, kernel_size=3, padding='same', kernel_regularizer=l2(0.01))(inputs)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)

    # Attention modules with increasing filters
    x = attention_module(x, 32)
    x = attention_module(x, 64)
    x = attention_module(x, 128)

    # Additional Conv1D layers with regularization
    x = Conv1D(256, kernel_size=3, padding='same', kernel_regularizer=l2(0.01))(x)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)

    # Global Max Pooling and Dense layers
    x = GlobalMaxPooling1D()(x)
    x = Dropout(0.6)(x)
    x = Dense(256, activation='relu', kernel_regularizer=l2(0.01))(x)
    x = Dropout(0.6)(x)
    outputs = Dense(num_classes, activation='softmax', kernel_regularizer=l2(0.01))(x)

    model = Model(inputs, outputs)
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

    return model

# Build and compile the model
input_shape = (X_train.shape[1], 1)
num_classes = y_train_categorical.shape[1]
model = build_attention_cnn(input_shape, num_classes)
model.summary()

# Early stopping callback with reduced patience
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

# Learning rate scheduler callback
lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)

# Train the model
history = model.fit(
    X_train, y_train_categorical,
    validation_data=(X_test, y_test_categorical),
    epochs=200,
    batch_size=32,
    verbose=1,
    callbacks=[early_stopping, lr_scheduler]
)

# Evaluate the model on the test set
test_loss, test_accuracy = model.evaluate(X_test, y_test_categorical, verbose=0)
print(f"Test accuracy: {test_accuracy:.4f}")

# Save the model
model_save_path = '/save/ecg_attention_cnn_model.h5'
model.save(model_save_path)
print(f"Model saved to {model_save_path}")

# Save the LabelEncoder
le_save_path ='/save/label_encoder.joblib'
joblib.dump(le, le_save_path)
print(f"LabelEncoder saved to {le_save_path}")

# Save input shape information
input_info = {
    'input_shape': X_train.shape[1],
    'num_classes': num_classes
}

input_info_path = '/save/input_info.json'
with open(input_info_path, 'w') as f:
    json.dump(input_info, f)
print(f"Input information saved to {input_info_path}")

# Plot the training and validation accuracy
plt.figure(figsize=(8, 6))
plt.plot(history.history['accuracy'], 'b-', label='Training Accuracy')
plt.plot(history.history['val_accuracy'], 'r-', label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.grid()
plt.savefig('accuracy_plot.png')

# Plot the training and validation loss
plt.figure(figsize=(8, 6))
plt.plot(history.history['loss'], 'b-', label='Training Loss')
plt.plot(history.history['val_loss'], 'r-', label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid()
plt.savefig('loss_plot.png')

# Make predictions on the test data
y_pred_categorical = model.predict(X_test)
y_pred = np.argmax(y_pred_categorical, axis=1)
y_true = np.argmax(y_test_categorical, axis=1)

# Compute confusion matrix
cm = confusion_matrix(y_true, y_pred)

# Plot confusion matrix
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=le.classes_)
disp.plot(cmap=plt.cm.Blues)
plt.title('Confusion Matrix')

# Adjust label rotation and layout to prevent overlap
plt.xticks(rotation=45, ha='right', fontsize=10)
plt.yticks(fontsize=10)
plt.tight_layout()
plt.savefig('confusion_matrix.png')
plt.show()