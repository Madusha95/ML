# -*- coding: utf-8 -*-
"""Day 03.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JlFRQtAB5VTLkaRc8DrKG_icX0Nwl6Fy

# Plan for Today,

1. Introduction to Feed Forward Neural Networks
2. Forward Propagation
3. Loss Optimization
4. Gradient Descent Algorithm
5. Optimizers, Adaptive Optimizers
6. Backpropagation Algorithm
7. Introduction to Tensorflow and Keras

# Deep Learning with Tensorflow and Keras
![image.png](attachment:image.png)

![1.png](attachment:1.png)

In 2006 was the discovery of techniques for learning in so-called deep neural networks. These techniques are now known as deep learning. They've been developed further, and today deep neural networks and deep learning achieve outstanding performance on many important problems in computer vision, speech recognition, and natural language processing!

“In a neural network we don't tell the computer how to solve our problem. Instead, it learns from observational data, figuring out its own solution to the problem at hand.”

## Applications

1. Face Recognition
2. Image Classification
3. Speech Recognition
4. Text-to-speech Generation
5. Handwriting Transcription
6. Machine Translation
7. Medical Diagnosis
8. Self Driving Cars
9. Digital Assistants
10. Ads, search, social recommendations

## History In Brief

1. 1943: Portrayed with a simple electrical circuit by neurophysiologist Warren McCulloch and mathematician Walter Pitt
2. 1950-1960: Perceptrons were developed by the scientist Frank Rosenblatt!
![image.png](attachment:image.png)
3. 1974-86: Backpropagation Algorithm, Recurrent NL
4. 1989-98: Convolutional Neural Networks, Bi Directional RNN, Long Short Term Memory (LSTM), MNIST Data Set
5. 2006: “Deep Learning” Concept
6. 2013 - 2014: Genarative Adverserial Networks / Deep Q Nets

# They use Deep Learning
![image.png](attachment:image.png)

![Picture1.png](attachment:Picture1.png)

# Algorithm (Neural Network Types)
![Picture3.png](attachment:Picture3.png)

# Today's Focus

![Picture2.png](attachment:Picture2.png)

# Supervised Learning Model

![w.png](attachment:w.png)

![Pictures1.png](attachment:Pictures1.png)

# Classification & Regression

1. <b>Classification:</b> “Classification" indicates that the data has discrete class label.
Classification predictive modeling is the task of approximating a mapping function (f) from input variables (X) to discrete output variables (y) or classes.
The output variables are often called labels or categories. The mapping function predicts the class or category for a given observation

2. <b>Regression: </b>Regression predictive modeling is the task of approximating a mapping function (f) from input variables (X) to a continuous output variable (y).
A continuous output variable is a real-value, such as an integer or floating point value. These are often quantities, such as amounts and sizes.
For example, a house may be predicted to sell for a specific dollar value, perhaps in the range of $100,000 to $200,000.

# Feed Forward neural Networks and Atrchitecture
![Picturess1.png](attachment:Picturess1.png)

# Lets train a FFNN for Rain Problem
![image.png](attachment:image.png)

# FFNN Architecture

![NN1.png](attachment:NN1.png)

# Trainable Parameters - Weights and Baises

![NN2.png](attachment:NN2.png)

Weights and biases (commonly referred to as w and b) are the learnable parameters of a machine learning model. They are variables and real numbers

1. Weights: All the Nets have weights
2. Bias : All the Neurons except neurons in the input layer

# How Neural Networks Train
![NN3.png](attachment:NN3.png)

## A Perceptron/ Neuron Generally,
![NN4.png](attachment:NN4.png)

# Neural Network gives a Predicition
"""

import numpy as np

w13=0.06
w14=0 #+0.05
w15=0.07

w23=0.06
w24=-0.01
w25=-0.06

w36=0.05
w46=-0.04
w56=0.08

w37=0.02
w47=0.01
w57=0.09

b3=0
b4=0
b5=0

b6=0
b7=0

"""### Layer 1 Weights
![image.png](attachment:image.png)
"""

WL1=np.array([[w13,w23],
              [w14,w24],
              [w15,w25]])

bL1=np.array([b3,b4,b5]).reshape(-1,1)

print(WL1,WL1.shape)
print(bL1,bL1.shape)

x1=37
x2=40

Xs=np.array([x1,x2]).reshape(-1,1)
print(Xs,Xs.shape)

YL2=np.matmul(WL1,Xs)+bL1
print(YL2)

"""### Inputs"""

x1=37
x2=40

Xs=np.array([x1,x2]).reshape(-1,1)
print(Xs,Xs.shape)

"""### Forward Propagation"""

print(WL1)
print(Xs)

#Layer 2

x1=37
x2=40

YL2=np.matmul(WL1,Xs)+bL1
print(YL2)

"""### Layer 2 Weights
![image.png](attachment:image.png)
"""

WL2=np.array([[w36,w46,w56],
             [w37,w47,w57]])

bL2=np.array([b6,b7]).reshape(-1,1)

print(WL2,WL2.shape)

#Layer 3 - Output Layer

YL3=np.matmul(WL2,YL2)+bL2
print(YL3)

"""[[0.4842]
 [0.2535]]

[[0.4287]
 [0.124 ]] #w13-> +0.05

[[0.2622]
 [0.1055]]#w23-> +0.05

## Actual vs Predicted
![image.png](attachment:image.png)

# Needed to be Improved / Trained

![image.png](attachment:image.png)

# Error / Loss
![image.png](attachment:image.png)

![image.png](attachment:image.png)
"""

error=(np.power(0-YL3[0],2)+np.power(1-YL3[1],2))/2
print(error)

"""### Error Regression Problems (Mean Squared Error)

![image.png](attachment:image.png)

### Error Classification Problems (Crossentropy Loss)

![image.png](attachment:image.png)

![opt.png](attachment:opt.png)

## Weight and Biase Adjustment - Gradient Decent Algorithm
![image.png](attachment:image.png)

# Gradient? Lets Recall
![NN5.png](attachment:NN5.png)

![image.png](attachment:image.png)

# We Also have a function E vs W
![NN6.png](attachment:NN6.png)

# The effect of Learning Rate
![image.png](attachment:image.png)

![image.png](attachment:image.png)

![image-2.png](attachment:image-2.png)

# How to Calculate the Gradient - Backpropagation Algorithm
![image.png](attachment:image.png)

# A Simple Example

![NN8.png](attachment:NN8.png)

## Now What?
![NN7.png](attachment:NN7.png)

# Stohastic Gradient Descent
![image.png](attachment:image.png)

# Activation Function
![image.png](attachment:image.png)

![image.png](attachment:image.png)

![image.png](attachment:image.png)

![image.png](attachment:image.png)

![image.png](attachment:image.png)

![image.png](attachment:image.png)

![image.png](attachment:image.png)

![image.png](attachment:image.png)

![image.png](attachment:image.png)

# Optimizers

![image.png](attachment:image.png)

![image.png](attachment:image.png)

![image.png](attachment:image.png)

![image.png](attachment:image.png)

![image.png](attachment:image.png)

## Lets take an example problem- Iris Flower example

The Iris flower data set or Fisher's Iris data set is a multivariate data set introduced by the British statistician, eugenicist, and biologist Ronald Fisher in his 1936 paper The use of multiple measurements in taxonomic problems as an example of linear discriminant analysis. It is sometimes called Anderson's Iris data set because Edgar Anderson collected the data to quantify the morphologic variation of Iris flowers of three related species. Two of the three species were collected in the Gaspé Peninsula "all from the same pasture, and picked on the same day and measured at the same time by the same person with the same apparatus". Fisher's paper was published in the journal, the Annals of Eugenics, creating controversy about the continued use of the Iris dataset for teaching statistical techniques today.

The data set consists of 50 samples from each of three species of Iris (Iris setosa, Iris virginica and Iris versicolor). Four features were measured from each sample: the length and the width of the sepals and petals, in centimeters. Based on the combination of these four features, Fisher developed a linear discriminant model to distinguish the species from each other.

## labels
![1_7bnLKsChXq94QjtAiRn40w.png](attachment:1_7bnLKsChXq94QjtAiRn40w.png)

## Features
![versicolor.jpg](attachment:versicolor.jpg)
## Dataset
(https://en.wikipedia.org/wiki/Iris_flower_data_set)

## Neural Network Architecture
![Untitled%20Diagram.png](attachment:Untitled%20Diagram.png)

## Training the FFNN
![Untitled%20Diagsram.png](attachment:Untitled%20Diagsram.png)
"""

import pandas as pd

dataset=pd.read_csv('iris.csv').values

data=dataset[:,0:4]
target=dataset[:,4]

from keras.models import Sequential
from keras.layers import Dense

model=Sequential()
#an empty NN created

model.add(Dense(64,input_dim=4,activation='relu'))
model.add(Dense(128,activation='relu'))
model.add(Dense(64,activation='relu'))
model.add(Dense(3,activation='softmax'))

model.compile(loss='categorical_crossentropy',optimizer='sgd',metrics=['accuracy'])
model.summary()

from keras.utils import np_utils

new_target=np_utils.to_categorical(target)
print(new_target[:5])

from sklearn.model_selection import train_test_split

train_data,test_data,train_target,test_target=train_test_split(data,new_target,test_size=0.1)

history=model.fit(train_data,train_target,epochs=100)

"""# Evaluating the Algorithm
![9.png](attachment:9.png)
"""

from matplotlib import pyplot as plt

plt.plot(history.history['loss'])

plt.plot(history.history['accuracy'])

predicted_target=model.predict(test_data)

print('Actual results:',test_target)
print('Predicted results:',predicted_target)

import numpy as np

print('Actual results:',np.argmax(test_target,axis=1))
print('Predicted results:',np.argmax(predicted_target,axis=1))

pip install tensorflow keras