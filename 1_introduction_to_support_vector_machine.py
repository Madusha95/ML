# -*- coding: utf-8 -*-
"""1-Introduction to Support Vector Machine.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1m0WGq9PiIMWBaBP5ow4cwJqmtfobUt5v

# Plan for Today

1. KNN Working Principal
2. Introduction to Support Vector Machine
3. SVM Kernels
4. Tutorial - Face Recognition pipeline using SVM and PCA

# Support Vector Machine

1. SVM offers very high accuracy compared to other classifiers such as KNN,and decision trees.
2. Very good in handling non linear input spaces.
3. Used in a variety of applications such as face detection,intrusion detection,classification of emails,news articles and webpages,classification of genes,and handwriting recognition.

# SVM Working Principal

1. The SVM classifier separates data points using a hyperplane with the largest amount of margin.
2. Also known as a discriminative classifier.
3. SVM finds an optimal hyperplane which helps in classifying new data points.
4. Can be employed in both types of classification and regression problems
5. It can easily handle multiple continuous and categorical variables. SVM constructs a hyperplane in multidimensional space to separate different classes
6. SVM generates optimal hyperplane in an iterative manner, which is used to minimize an error.
7. The core idea of SVM is to find a maximum marginal hyperplane(MMH) that best divides the dataset into classes.
![image.png](attachment:image.png)

## Support Vectors
1. Support vectors are the data points, which are closest to the hyperplane.
2. These points will define the separating line better by calculating margins. These points are more relevant to the construction of the classifier.

## Hyperplane
A hyperplane is a decision plane which separates between a set of objects having different class memberships

## Margin
1. A margin is a gap between the two lines on the closest class points. This is calculated as the perpendicular distance from the line to support vectors or closest points.
2. If the margin is larger in between the classes, then it is considered a good margin, a smaller margin is a bad margin.

## How does SVM work?
1. The main objective is to segregate the given dataset in the best possible way.
2. The distance between the either nearest points is known as the margin.
3. The objective is to select a hyperplane with the maximum possible margin between support vectors in the given dataset

SVM searches for the maximum marginal hyperplane in the following steps,

1. Generate hyperplanes which segregates the classes in the bestway
2. Select the right hyperplane with the maximum segregation from the either nearest datapoints as shown in the figure.
![image.png](attachment:image.png)

## Selecting the best hyperplane

![image.png](attachment:image.png)

### Few possible hyperplanes for the earlier figure
![image.png](attachment:image.png)

### Select the Maximum Margin
![image.png](attachment:image.png)

![image.png](attachment:image.png)

## What about Linear inseperable datasets
![image.png](attachment:image.png)
"""

import pandas as pd

dataset=pd.read_csv('linear-insperable-dataset.csv').values

data=dataset[:,0:2]
target=dataset[:,2]

from matplotlib import pyplot as plt

x=data[:,0]
y=data[:,1]

for i in range(len(data)):
    if(target[i]==1):
        plt.scatter(x[i],y[i],c='r')
    else:
        plt.scatter(x[i],y[i],c='g')
plt.show()

from sklearn.model_selection import train_test_split

train_data,test_data,train_target,test_target=train_test_split(data,target,test_size=0.1)

from sklearn.svm import SVC

model=SVC(kernel='linear')
model.fit(train_data,train_target)

predicted_target=model.predict(test_data)

from sklearn.metrics import accuracy_score

acc=accuracy_score(test_target,predicted_target)
print('Accuracy:',acc)

"""## Adding a higer dimension"""

import numpy as np

z=2*np.power(x,2)+3*np.power(y,2)+3 #z=2x^2+3y^2

from matplotlib import pyplot as plt

for i in range(len(data)):
    if(target[i]==1):
        plt.scatter(x[i],z[i],c='r')
    else:
        plt.scatter(x[i],z[i],c='g')

    plt.xlabel('X vals')
    plt.ylabel('Z vals')
plt.show()

data[:,0]=x
data[:,1]=z

train_data,test_data,train_target,test_target=train_test_split(data,target,test_size=0.1)

model=SVC(kernel='linear')
model.fit(train_data,train_target)

predicted_target=model.predict(test_data)

acc=accuracy_score(test_target,predicted_target)
print('Accuracy:',acc)

"""![image.png](attachment:image.png)

# Solution- SVM Kernels

Where SVM becomes extremely powerful is when it is combined with kernels. There we projected our data into higher-dimensional space defined by polynomials and Gaussian basis functions, and thereby were able to fit for nonlinear relationships with a linear classifier.SVM algorithms use a set of mathematical functions that are defined as the kernel. The function of kernel is to take data as input and transform it into the required form. Different SVM algorithms use different types of kernel functions. These functions can be different types. For example linear, nonlinear, polynomial, radial basis function (RBF), and sigmoid.
Introduce Kernel functions for sequence data, graphs, text, images, as well as vectors. The most used type of kernel function is RBF. Because it has localized and finite response along the entire x-axis.
The kernel functions return the inner product between two points in a suitable feature space. Thus by defining a notion of similarity, with little computational cost even in very high-dimensional spaces.

### Kernel Types

1. Polynomial
2. RBF
3. Gausian Basis
4. Sigmoid
5. Tanh
![image.png](attachment:image.png)
"""

dataset=pd.read_csv('linear-insperable-dataset.csv').values

data=dataset[:,0:2]
target=dataset[:,2]

from matplotlib import pyplot as plt

x=data[:,0]
y=data[:,1]

for i in range(len(data)):
    if(target[i]==1):
        plt.scatter(x[i],y[i],c='r')
    else:
        plt.scatter(x[i],y[i],c='g')
plt.show()

from sklearn.svm import SVC

#model=SVC(kernel='poly',degree=2)
model=SVC(kernel='rbf')
model.fit(train_data,train_target)

predicted_target=model.predict(test_data)

from sklearn.metrics import accuracy_score

acc=accuracy_score(test_target,predicted_target)
print('Accuracy:',acc)

